\section{Circuit-independent (universal) pairing-based SNARKs}
\noindent Homogeneous computation aside, circuit-specific pairing-based methods have a couple of noteworthy shortcomings. The first of these, as noted by Groth \cite{grothupdatable}, is that there is no way of ensuring the deployers of the SNARK in question have disposed of the secret randomness used to generate the trusted setup. The second and more important issue is a lack of flexibility due to the use of computation-dependent (non-universal) preprocessing. Consider the groth16 trusted setup for instance: 
\begin{align}
\sigma = \Big( 
    &[\alpha]_1, [\beta]_1, [\gamma]_1, [\delta]_1, \{[x^i]_1\}_{i \in [0, n-1]}, \{[\gamma^{-1}(\beta u_i(x) + \alpha v_i(x) + w_i(x))]_1\}_{i \in [0, \ell]}, \\
    &\{[\delta^{-1}(\beta u_i(x) + \alpha v_i(x) + w_i(x))]_1\}_{i \in [\ell+1, m]}, \\
    &\{[\delta^{-1} x^i t(x)]_1\}_{i \in [0, \deg(h)]}, [\beta]_2, [\gamma]_2, [\delta]_2, \{[x^i]_2\}_{i \in [0, n-1]}
\Big)
\end{align}

\noindent As is the case with other discussed methods, the contributions of the $i$-th witness component to gate $g$ are ``stuck'' in the encodings of $u_i(x), v_i(x), w_i(x)$ (not the component's value, but its index). Thus if the circuit structure changes, the setup terms encoding the hidden combinations of the $v_i(x), w_i(x), y_i(x), \text{etc.}$ must be regenerated. A natural question arises: why not just update the setup somehow if the computation structure changes? This is not possible for two reasons, either of which is sufficient to deter updatability. The first is that, as mentioned the hidden $u_i(x), v_i(x), w_i(x)$ are interpolated from relationships between witness values (secret) and the gates they feed into. Adding a new gate and its associated wire connections would require adding a new data point to each of these polynomials. Thus, one would have to re-interpolate all of them which is not possible without violating cryptographic assumptions (since they are encrypted) or regenerating the entire setup, which defeats the purpose of updating anything. The other reason concerns the setup's combination of these hidden polynomial terms, such as the $[\beta u_i(x) + \alpha v_i(x) + w_i(x)]_1$. Groth \cite{grothupdatable} showed these setup terms could be used to extract the the constituent monomials and ultimately break soundness if the setup allowed updates. These observations suggest that the conception of a QAP-based updatable SNARK is unlikely, and that different approaches are necessary. Such approaches should be \textit{universal}, in the sense that they allow proving computation of any structure up to a certain size; and \textit{updatable}, meaning that the trusted setup can be efficiently updated by any party and remains sound if at least one setup contributor is honest.\\

\noindent To address these shortcomings, Groth \cite{grothupdatable} produced a multivariate scheme that encoded QAP elements differently and achieved constant proof size and verification complexity. Although this scheme involved a linear-time procedure by which a linear-size circuit-specific setup could be produced as needed, true universality required quadratically many terms w.r.t circuit size. Furthermore, SRS updates take a quadratic number of group exponentiations and update verification a linear number of pairing operations. For circuits with millions of gates this is impractical. However, this hints that a SNARK with linear-size universal \& efficiently updatable setup could be attainable if the setup terms are univariate (and monomial).\\

\noindent Maller et al. achieved this with Sonic, which draws inspiration from techniques of Bootle et al. \cite{bootlezkargs} reducing circuit satisfiability to checking Laurent polynomials encoding a Hadamard matrix product (models mul. gate operations) paired with a linear constraint system (models wire connections \& addition gates). Though they take up more space due to the presence of $X^{-i}$ term for every $X^i$ term (loosely speaking), the setup contains only univariate monomials and therefore requires linear space. Being composed of monomials, the setup can be updated by any party, for which they supply a proof of correctness. In a realistic deployment of this scheme, the setup would not be circuit dependent, and a single update could eliminate the risk that the deployers still hold valid toxic waste that can be used to subvert the setup. Maller et al. improved upon this with Marlin \cite{marlin} a more verifier-succinct approach which cryptographically compiles a ``sparsity-friendly'' polynomial representation of an R1CS instance into familiar pairing checks. Within, both methods make use of bi-variate and univariate KZG commitments, respectively; more importantly, they also make use of \textit{permutation arguments}, which prove the correct copying of values in consecutive circuit wires. \textbf{maybe say why these details are important, instead of randomly mentioning them}\\

\noindent Though significant in their own right, Sonic and Marlin suffer large constants in proof construction complexity despite being asymptotically quasilinear, with Sonic more negatively affected in this regard. A likely cause for Sonic's shortcoming here is the attempt to accommodate for $n$-fan-in circuits whose gates can accept arbitrarily many inputs. While a reasonable generalization, this allows any given linear constraint the ability to use arbitrarily many witness inputs, requiring the use of entire witness and selector vectors for each constraint. It also causes a bloated permutation argument since a given gate may require arbitrarily many copy constraints between its own inputs and outputs from preceding gates that feed into it. Gabizon et al. instead used a simpler arithmetization combined with structural assumptions about the number of inputs each circuit gate receives (2 inputs per gate). Here, each gate constraint can express any gate, and their permutation argument only requires a constant number of permutation check per gate due to the 2-fan-in assumption. Combining these traits with randomized interaction from the verifier yields a polynomial divisibility relation involving a random linear combination of the polynomials encoding the gate relations and copy constraints. This can be checked using univariate KZG commitment scheme, marking the use of pairing-based cryptography. The resulting scheme is known as ``PlonK'', where the ``L'' refers to ``Lagrange bases'' consisting of sparsely represented Lagrange polynomials defined over the roots of unity. These polynomials are used in the interpolation and aggregation of the various polynomials involved in proof construction.\\

\noindent The combination of flexibility, intuitive arithmetization, and better efficiency has made PlonK a catalyst for advancement.

\noindent Here is a table:
\input{tex_files/snark-table}
