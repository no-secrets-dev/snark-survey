\section{Circuit-independent (universal) pairing-based SNARKs}
\noindent Homogeneous computation aside, circuit-specific pairing-based methods have a couple of noteworthy shortcomings. The first of these as noted by Groth \cite{grothupdatable} is that there is no way of ensuring the deployers of the SNARK in question have disposed of the secret randomness used to generate the trusted setup. The second and more important issue is a lack of flexibility due to the use of computation-dependent (non-universal) preprocessing. As an example, we depict the groth16 trusted setup here. As in the QAP definition, the $u_i(x), v_i(x), w_i(x)$ encode the contribution of the $i$-th witness component to left input, right input, and output of gate $x$ respectively. 
\begin{align}
\sigma = \Big( 
    &[\alpha]_1, [\beta]_1, [\gamma]_1, [\delta]_1, \{[x^i]_1\}_{i \in [0, n-1]}, \{[\gamma^{-1}(\beta u_i(x) + \alpha v_i(x) + w_i(x))]_1\}_{i \in [0, \ell]}, \\
    &\{[\delta^{-1}(\beta u_i(x) + \alpha v_i(x) + w_i(x))]_1\}_{i \in [\ell+1, m]}, \\
    &\{[\delta^{-1} x^i t(x)]_1\}_{i \in [0, \deg(h)]}, [\beta]_2, [\gamma]_2, [\delta]_2, \{[x^i]_2\}_{i \in [0, n-1]}
\Big)
\end{align}

\noindent As is the case with other discussed methods, the contributions of the $i$-th witness component to gate $x$ are ``stuck'' in the encrypted $u_i(x), v_i(x), w_i(x)$ (not the component's value, but its index). Thus if the circuit wiring changes, these polynomials must change to reflect this. Updating the setup incrementally, however, is not possible for two reasons, either of which is sufficiently limiting. The first is the hidden $u_i(x), v_i(x), w_i(x)$ are interpolated from relationships between witness components and the gates they feed into. Adding a new gate and its associated wire connections would require adding a new data point to each of these polynomials. Thus, one would have to re-interpolate all of them which is not possible without violating cryptographic assumptions (since they are encrypted) or regenerating the entire setup, which defeats the purpose of updates. Another related reason concerns the setup's combination of these hidden polynomial terms, such as the $[\beta u_i(x) + \alpha v_i(x) + w_i(x)]_1$. Groth \cite{grothupdatable} showed these setup terms could be used to extract the the constituent monomials and ultimately break soundness if the setup allowed updates. These observations suggest that the conception of a QAP-based updatable SNARK is unlikely, and that different approaches are necessary. Such approaches should be \textit{universal}, in the sense that they allow proving computation of any structure up to a certain size; and \textit{updatable}, meaning that the trusted setup can be efficiently updated by any party and remains sound if at least one setup contributor is honest. This idea becomes highly relevant when deploying blockchain systems using SNARKs with trusted setups. After one update to the setup (which can be performed by anyone with a corresponding proof), users do not have to worry about the network being subverted by its deployers or other adversaries. As they stray from the QAP-based paradigm which seems to be ``maxed out'', the following methods use a more diverse set of arithmetizations and polynomial checking to fulfill these universality/updatability related goals; consequently, these are where most of the notable points of comparison arise. For this reason, our discussion of security is significantly more sparse in the following sections; that said, the most noteworthy methods prove security in the algebraic group model (AGM) \cite{agm}, in which adversarial algorithms can exploit group-specific structure (weaker security assumptions than than GGM). \\

\noindent To meet universality/updatability-related ends, Groth \cite{grothupdatable} produced a universal and updatable scheme using a multivariate polynomial encoding of QAP elements. Although this scheme had constantn proof size, constant verification complexity, and a linear-time procedure by which a linear-size circuit-specific setup could be produced as needed, true universality required quadratically many terms w.r.t circuit size. Furthermore, SRS updates would take a quadratic number of group exponentiations and update verification a linear number of pairing operations. For circuits with millions of gates this is impractical. However, this attempt hints that a SNARK with linear-size universal \& efficiently updatable setup could be attainable if the setup terms are univariate (and monomial). Maller et al. achieved this with Sonic, which draws inspiration from techniques of Bootle et al. \cite{bootlezkargs} reducing circuit satisfiability to checking Laurent polynomials encoding a Hadamard matrix product (models mul. gate operations) paired with a linear constraint system (models wire connections \& addition gates). They also use a permutation argument inspired by Bootle et al. \cite{grothshuffle} to enforce correct copying of values across linked wires. The setup contains only univariate monomials and therefore requires linear space, though it is twice the space one would like due to the presence of $X^{-i}$ term for every $X^i$ term (loosely written). Being composed of monomials, the setup can be updated by any party supplying a corresponding proof of update validity. In a realistic deployment of this scheme, the setup would not be circuit dependent, and a single update could eliminate the risk that the deployers still hold valid setup secrets.\\ 

\noindent Though significant in its own right, Sonic suffers from two shortcomings. First, it is not fully verifier-succinct. Second and foremost, there are large constants in the proof construction complexity. A likely cause is the attempt to accommodate for $n$-fan-in circuits whose gates can accept arbitrarily many inputs. While a reasonable generalization, this allows any given linear constraint the ability to use arbitrarily many witness inputs, requiring the use of entire witness and selector vectors for each constraint. It also causes a bloated permutation argument since a given gate may require arbitrarily many copy constraints between its own inputs and outputs from preceding gates that feed into it. Gabizon et al. \cite{plonk} addressed these issues with a simplifying assumption that may not catch one's eye, but turns out to have important performance implications: simply let each circuit gate take two inputs. The impact of this is twofold. Firstly, it enables a much simpler constraint representation of arithmetic circuit-SAT, where for an $n$ gate circuit we have the following constraint for gate $i$. Here $a_i, b_i, c_i$ the indices in $\mathbf{x}$ corresponding to the left input, right input, and output of the $i$-th gate, respectively; and $\mathbf{q_L}, \mathbf{q_R},\mathbf{q_O},\mathbf{q_M},\mathbf{q_C},$ are ``selector vectors'' determining which gate-related values partake in the constraint. This allows one to express addition or multiplication gates in the same constraint by setting $(\mathbf{q_L})_i, (\mathbf{q_R})_i, (\mathbf{q_M})_i$ accordingly. 
\begin{align}
(\mathbf{q_L})_i \cdot \mathbf{x}_{(\mathbf{a})_i} + (\mathbf{q_R})_i \cdot \mathbf{x}_{(\mathbf{b})_i} + (\mathbf{q_O})_i \cdot \mathbf{x}_{(\mathbf{c})_i} + (\mathbf{q_M})_i \cdot (\mathbf{x}_{(\mathbf{a})_i} \cdot \mathbf{x}_{(\mathbf{b})_i}) + (\mathbf{q_C})_i = 0
\end{align}

    % &\Leftrightarrow \textbf{q}_{a} \circ \textbf{a} + \textbf{q}_{b} \circ \textbf{b} + \textbf{q}_{M} \circ (\textbf{a} \circ \textbf{b}) + \textbf{q}_{C} \circ \textbf{c} = \textbf{0}

\noindent During proof construction, these terms are collected into polynomials $q_L(x), a(x), q_R(x), b(X),$ etc. and combined in the same manner as the original constraint equation. Secondly, the 2-fan-in assumption enables optimizations to the permutation argument inspired by Groth et al. and Maller et al. \cite{grothshuffle, sonic}. While sonic used a product check accounting for the arbitrarily many copy constraints per gate, each product check per gate involves three terms in the numerator and denominator each, resulting in a ``grand product'' used in proof construction. We show the non-zero-knowledge version here, but adding ZK properties requires shifting this equation by adding a ``masking'' polynomial that still vanishes on the desired domain in order to not corrupt the expression.
\begin{align}
    z(X) &= L_1(X) + \\
    &\sum_{i=1}^{n-1} L_{i+1}(X) \prod_{j=1}^{i} \frac{(w_j + \beta\omega^j + \gamma)(w_{n+j} + \beta k_1\omega^j + \gamma)(w_{2n+j} + \beta k_2\omega^j + \gamma)}{(w_j + \beta\sigma^*(j) + \gamma)(w_{n+j} + \beta\sigma^*(n+j) + \gamma)(w_{2n+j} + \beta\sigma^*(2n+j) + \gamma)}
\end{align}

\noindent where $L_i(x) = \frac{X^n - 1}{X - \omega^i}$ is the $i$-th Lagrange basis polynomial defined over the $n$-th roots of unity (gives 1 when $X = \omega^i$ and 0 otherwise). Notably, the polynomial check works well with a common optimization to PlonK's invocation of KZG scheme; by representing the hidden monomials in the Lagrange basis, the point evaluations of polynomials can be computed in $O(d)$ operations where $d$ is the degree of the polynomial in question, which is faster than the $O(d \log d)$ complexity for interpolation via inverse FFT \& subsequent evaluation. The pairing check performed by the verifiere is also ``fixed argument'' in the second source group, enabling both of only 2 pairings the verifier performs to be 30\% faster than the traditional pairing \cite{fapairings}.\\

\noindent The combination of flexibility, intuitive arithmetization, and better efficiency has made PlonK a gateway to other methods increasing expressiveness and performance, with important implications for so-called ``zero-knowledge virtual machines'' (zkVMs). Ambrona et al. \cite{turboplonk} proposed methods to optimize the constraint system used, as well as optimized circuits for PlonK-based verifiable implementations of the ``SNARK-friendly'' Poseidon hash function \cite{poseidon}. For zkVM operations less compatible with SNARK systems, Gabizon et al. proposed Plookup \cite{plookup} which adapts the PlonK permutation argument to verify that a set of values is present in some predetermined table; this is particularly useful for constraining the correctness of zkVM operations involving nonlinear operations that would make the resulting circuits / constraint system inefficient to verify. A notable example is a ``SNARK-unfriendly'' hash function like SHA-3 \cite{sha3}, which has many nonlinear bit-mixing operations. Variations like halo2 \cite{halo2} and plonky2 \cite{plonky2} combine the PlonK arithmetization with commitment schemes inspired by Bulletproofs \cite{bulletproofs} and FRI \cite{fri} respectively to shed the need for a trusted setup at the expense of slower verification. These two advancements highlight the customizability of the PlonK system, which is futher explored among other methods in the following sections.

% Chiesa et al. improved upon this with Marlin \cite{marlin}, a more verifier-succinct approach which cryptographically compiles a ``sparsity-friendly'' polynomial representation of an R1CS instance for the verifier to check algebraic consistency of via pairings. Within, both methods make use of bi-variate and univariate KZG commitments, respectively; more importantly, they also make use of \textit{permutation arguments}, which prove the correct copying of values in consecutive circuit wires. As a key occurrence of flexibility-performance tradeoff, the latter sub-argument is necessary due to the encoding of witness-gate relationshiphs directly in the proof instead of the setup.\\
